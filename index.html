<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0042)https://research.nvidia.com/labs/par/calm/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><script src="" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: #666;
    text-align: left;
    margin-top: 8px;
    margin-bottom: 8px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}

.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 150px;
  font-weight: 600;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}

.kitti {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}



</style>


</head><body><div class="topnav" id="myTopnav">
    <figure style="width: 100%;">
        <a href="./files/Bar-Ilan_University_logo.svg.png">
            <img width="100%" src="./files/Bar-Ilan_University_logo.svg.png">
        </a>
    </figure>
</div>

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="./files/style/hidebib.js"></script>
<link href="./files/style/css" rel="stylesheet" type="text/css">

    <title>Lay-A-Scene: Personalized 3D Object Arrangement Using Text-to-Image Priors</title>
    <meta property="og:description" content="-Given a set of 3D objects, the task is to find a plausible arrangement of these objects in a scene.">
    <link href="./files/style/css2" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="./files/style/js"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>




 
<div class="container">
    <div class="paper-title">
      <h1>Lay-A-Scene: Personalized 3D Object Arrangement Using Text-to-Image Priors</h1>
    </div>

    
    <div id="authors">
        <div class="author-row">
            <div class="col-3 text-center"><a href="https://ohad204.github.io/ohadrahamim.github.io/">Ohad Rahamim</a><sup>1</sup></div>
            <div class="col-3 text-center"><a href="">Hilit Segev</a><sup>1</sup></div>
            <div class="col-3 text-center"><a href="https://idanachituve.github.io/">Idan Achituve</a><sup>1</sup></div>
            <div class="col-3 text-center"><a href="https://ykasten.github.io/">Yuval Atzmon</a><sup>2</sup></div>
            <div class="col-3 text-center"><a href="https://ykasten.github.io/">Yoni Kasten</a><sup>2</sup></div>
            <div class="col-3 text-center"><a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a><sup>1,2</sup></div>
        </div>

        <div class="affil-row">
            <div class="venue text-center"><sup>1</sup>Bar-Ilan University</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><sup>2</sup>NVIDIA</div>
        </div>
        <!-- <div class="affil-row">
            <div class="venue text-center"><b>NeurIPS 2024</b></div>
        </div> -->

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="">
                <span class="material-icons">  </span> 
                 Paper
            </a>
            <a class="supp-btn" href="">
                <span class="material-icons">  </span> 
                  Code (soon) 
            </a>
        </div></div>
    </div>

    <section id="teaser">
            <figure style="width: 100%;">
                <a href="./files/teaser.png">
                    <img width="100%" src="./files/teaser.png">
                </a>
                <p class="caption" style="margin-bottom: 1px;">
                    We present Lay-A-Scene: method for addressing the 3D-object-Arrangement task. The key idea is that since the objects already exist, we can use the text-to-image models to generate a single image that can serve as a layout to find the arrangement of the objects.
                </p>
            </figure>
    </section>

    <section id="abstract">
        <h2>Abstract</h2>
        <hr>
        <p>
            Generating 3D visual scenes is at the forefront of visual generative AI, but current 3D generation techniques struggle 
            with generating scenes with multiple high-resolution objects. Here we introduce Lay-A-Scene, which solves a task of Open-set 3D Object Arrangement, 
            effectively arranging unseen objects. Given a set of 3D objects, the task is to find a plausible arrangement of these objects in a scene. 
            We address this task by leveraging pre-trained text-to-image models. 
            We personalize the model and explain how to generate images of a scene that contains multiple predefined objects without neglect. 
            Then, we describe how to infer the 3D poses and arrangement of objects from a 2D generated image by finding a consistent projection of objects 
            onto the 2D scene. We evaluate the quality of Lay-A-Scene using 3D objects from Objaverse and human raters and find that it often generates
             coherent and feasible 3D object arrangements. 
        </p>
    </section>

    <hr>
    <h1>Overview</h1>
    <hr>

    <section id="teaser-videos">
        <div class="flex-row">
        <figure style="width: 100%;">
            <video width="90%" controls="" muted="" loop="" autoplay="">
                <source src="./files/00034.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
            <div style="width: 100%;">
                <br><br>
                <p>
                    In many cases, one may have access to existing models of 3D objects and is only interested in finding plausible arrangements 
                    of given objects. This setup, which we call here Openset-3D-Arrange, 
                    can be viewed as a 3D parallel of the personalization problem in image generation. 
                    It can also be viewed as a complementing problem to scene-generation papers, which generate objects in a scene given a layout. 
                    3D-Arrange is about the reverse problem -- find a plausible layout for given 3D objects.  
                    3D object arrangement has been previously addressed by training models using a specific training dataset. 
                    The question remains if this problem can be solved by distilling information from current text-to-image models.
                </p> 

                <p>
                    Here we describe Lay-A-Scene, a method for addressing the 3D-object-Arrangement task. The key idea is that since the 
                    objects already exist, we can use the text-to-image models to generate a single image that can serve as a layout to 
                    find the arrangement of the objects. Instead of multiple inferences of those models to generate a full scene, 
                    we generate an image with a plausible layout using a single forward pass. Later, we match each object and its 
                    appearance in the generated image to infer the object's position. Those positions with the objects create a full scene.
                </p>

                <p>
                    Lay-A-Scene approach has several main advantages. 
                    Unlike other 3D-arrange methods it can handle new objects through 
                    personalization without retraining the foundation model. 
                    Unlike graph-based 3D arrangement methods, which expect users to provide a set of spatial relations, 
                    Lay-A-Scene generates the scene layout based on the prior learned by text-to-image diffusion models.
                </p>
                
                
                <p>
                    Several key challenges emerge when taking this approach approach. 
                    First, how could we infer 3D position of given objects from generated images? 
                    distorted or colliding. 
                    We show how to add prior information about physical considerations as soft constraints to the Prospective-n-Points optimization to 
                    generate scenes that are more coherent and natural. We call this approach Side-information-PNP,
                    and find that it greatly improves the generated scenes.  
                    We also note that Text-to-image models often suffer from entity neglect, 
                    where generated images do not contain all entities mentioned in their prompts. 
                    This problem becomes more severe when objects are unusual or incoherent, 
                    and when the number of objects grows.
                    In our setting, the PnP procedure can be used to filter out images with entity neglect.  
                </p>   
                
            </div>
        </div>
    </section>

    <section id="pipeline">

        <h2>Pipeline</h2>
        <hr>

        <figure style="width: 100%;">
            <a href="./files/workflow_22_5.png">
                <img width="100%" src="./files/workflow_22_5.png">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
                Lay-A-Scene consists of two phases. First, given objects are used to personalize a text-to-image model and a scene image is generated. In the second phase, we find a  transformation $T_i$ for each 3D object $i$ to match the 2D arrangement presented in the generated scene image. $T_i$ is found using our \ourpnp{}, by matching the DIFT representation of objects and scene image.
            </p>
        </figure>

        <h2>Results</h2>
        <hr>
        We evaluate our method and baselines on 3D furniture meshes taken from the Objaverse dataset, a repository featuring over 800K high-quality 3D assets. 
        <h2>Objaverse dataset</h2>
        <hr>
        
        <section id="teaser">
            <figure style="width: 100%;">
                <a href="./Example_titles.png">
                    <img width="100%" src="./files/Example_titles.png">
                </a>
            </figure>
        </section>

        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/01184.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 49%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/04797.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/09484.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        

        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/06127.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/06145.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        

        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/07306.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/09639.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/09424.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/04154.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="49.5%" controls="" muted="" loop="" autoplay="">
                <source src="./files/04457.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            
            <p class="caption" style="margin-bottom: 1px;">
            Example arranged in two columns, where each column has the following structure: 
                Left: The partial scans that are given as input to our model. Middle: the completed surface. Right: the completed surface together with the input points.
                 For more examples, please refer to the paper.
            </p>
        </figure>
        
        
        <h2>KITTI dataset</h2>
        <hr>
        
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/motorcycle5_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/car_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/motorcycle7_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>

        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/car2_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/motorcycle10_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/truck_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/truck7_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
        <figure style="display: inline; width: 33%;">
            <video style="display: inline;" width="24%" controls="" muted="" loop="" autoplay="">
                <source src="./files/KITTI/truck13_ours.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            
                        <p class="caption" style="margin-bottom: 1px;">
                The completed surface (gray) together with the input points (blue).
            </p>
        </figure>

    </section>
    
    <section id="bibtex">
        <h2>Citation</h2>
        <pre><code>
@article{kasten2023point,
  title={Point-Cloud Completion with Pretrained Text-to-image Diffusion Models},
  author={Kasten, Yoni and Rahamim, Ohad and Chechik, Gal},
  journal={arXiv preprint arXiv:2306.10533},
  year={2023}
}
        </code></pre>
    </section>

    

<br>


</div>


</body></html>
